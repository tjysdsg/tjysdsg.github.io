---
title:  "Understanding the Math Behind CTC"
date:   2021-05-29 18:05:00 +0800
excerpt_separator: <!--more-->
permalink: /ctc
---

In this post, I'm writing down my thought process of understanding the math behind 
Connectionist Temporal Classification (CTC) [TODO reference].

<!--more-->

This article assumes that you understand the intuition behind CTC.

# Definitions

## Input and Output Sequences

The input of a CTC network (an audio signal) is a sequence denoted as
$$
\boldsymbol{x} =
\begin{bmatrix}
x_1 & x_2 & \cdots & x_m \\
\end{bmatrix}
$$
while the target sequence (a list of phonemes) is
$$
\boldsymbol{z} =
\begin{bmatrix}
x_1 & x_2 & \cdots & x_n \\
\end{bmatrix}
$$.

The target sequence consists of any symbols drawn from an alphabet $L$.

Note that the length of $\boldsymbol{x}$, $T$, is larger than the length of
$\boldsymbol{z}$, $U$.

## Operations and Values

The output of the neural network, $\boldsymbol{y}$, is a matrix, where
$y_k^t$ is the probability of observing label $k$ at time $t$.
$\boldsymbol{y}$ consists of symbols drawn from another alphabet $L'$,
which has one more symbol called "blank", denoted as $\%$, than $L$.

Also, we define an operation $\mathcal{B}: L'^{T} \mapsto L^{\le T}$ that

1. Merge consecutive identical symbols
2. Remove the blank symbol

For example,
$$
\mathcal{B}(aaa\%b\%cc\%d) = abc
$$

## Derivation

### The Objective

Assuming the output $z_1,z_2,\dots$ is conditionally independence
w.r.t the input $\boldsymbol{x}$,
we define
$$
y = p(\boldsymbol{\pi}|\boldsymbol{x}) = \prod y_{\boldsymbol{\pi}_t}^{t}
$$
where $\boldsymbol{\pi} =
\begin{bmatrix}
\pi_1 & \pi_2 & \cdots & \pi_n \\
\end{bmatrix}
$ is a possible sequence that can be mapped to $\boldsymbol{z}$ by $\mathcal{B}$.

Then, the objective of CTC is to find a sequence $\boldsymbol{l}$ 
that maximize the probability

$$
p(\boldsymbol{l}|\boldsymbol{x}) = \sum_{\pi\in\mathcal{B}^{-1}}
p(\boldsymbol{\pi}|\boldsymbol{x})
$$

*in an efficient way*.

Here $L^{\le T}$ refers to a sequence that has a length smaller than $T$.

In other words, the objective of CTC is to find:

$$
h(x) = \arg\max_{l\in L^{\le T}} p(\boldsymbol{l}|\boldsymbol{x})
$$


### Training

To estimate $p(\boldsymbol{l}|\boldsymbol{x})$, we use a method similar to
Viterbi decoding or the forward-backward algorithm.
By doing this, we can calculate the *forward variable* and *backward variable*.
And we express the objective function using
them so that by minimizing the objective, the RNN learns to model the audio sequence.

#### Forward & Backward

For a sequence $\boldsymbol{q}$ with a length $r$,
the first $p$ of its elements are denoted by
$$
\boldsymbol{q}_{1:p}
$$
and the last $p$ of its elements are denoted by
$$
\boldsymbol{q}_{r-p:r}
$$

For some labeling $\boldsymbol{l}$ (a result of $\mathcal{B}$),
its *forward variable* at time $t$ for symbols before the $s$-th symbol is

$$
\alpha_t(s) = \sum_{
\begin{align}
    &\pi\in N^T \\
    &\mathcal{B}(\pi_{1:t}) = \boldsymbol{l}_{1:s} \\
\end{align}
}
\prod_{t'=1}^t y_{\pi_{t}'}^{t'}
$$

This basically means that the forward variable is the sum of probabilities of all paths that can
result in $\boldsymbol{l}_{1:s}$ at time $t$.

Similarly, *the backward variable* at time $t$ for symbols after the $s$-th symbol is defined as:

$$
\beta_t(s) = \sum_{
\begin{align}
&\pi\in N^T \\
&\mathcal{B}(\pi_{t:T}) = \boldsymbol{l}_{s:|\boldsymbol{l}|} \\
\end{align}
}
\prod_{t'=t}^T y_{\pi_{t}'}^{t'}
$$

#### Considering the "blank" symbol

The blank symbol makes things a little more difficult in terms of forward-backward algorithm and decoding.
There are some rules regarding to whether a transition from node A to node B is possible:

1. transitions between blank and non-blank labels are allowed, because transitions between blanks do not make
   any sense
2. transitions between any pair of *distinct* non-blank labels are allowed, because the blank symbol is used
   to separate consecutive same labels
3. the initial state is either a blank or the first symbol in $\boldsymbol{l}$

Also, during training, what we are actually performing the forward-backward algorithm on is a modified sequence
$\boldsymbol{l}'$, which has blanks added to the beginning and the end of $\boldsymbol{l}$ and blanks inserted
between every two labels.

Thus, the initialization of $\alpha$ and $\beta$ is:

$$
\begin{align}
    \alpha_1(1) &= y_b^1 \\
    \alpha_1(2) &= y_{\boldsymbol{l}_1}^1 \\
    \alpha_1(s) &= 0, \forall s > 2 \\
    \beta_T(|\boldsymbol{l}'|) &= y_b^T \\
    \beta_T(|\boldsymbol{l}'| - 1) &= y_{\boldsymbol{l}_{|\boldsymbol{l}|}}^T \\
    \beta_T(s) &= 0, \forall s < |\boldsymbol{l}'| - 1 \\
\end{align}
$$

#### Recursion

The major advantage of using the forward-backward algorithm is that you can calculate $\alpha$ or $\beta$
recursively, which means you can use dynamic programming to do it efficiently.
For example, when calculating the first n-th Fibonacci numbers, instead of performing recursion on every
$i\in [1, n]$, you can store the result of the previous results $f_{i-1}, f_{i-2}, f_{i-3}, \dots$, and calculate
$f_i$ by simply summing $f_{i-1}$ and $f_{i-2}$.

The formula for calculating $\alpha$ and $\beta$ recursively given in the paper is:

$$
\alpha_t(s) =
\begin{cases}
    \bar{\alpha}_t(s)y_{\boldsymbol{l}_{s}'}^t & \boldsymbol{l}_{s}' = b \text{ or } \boldsymbol{l}_{s-2}' = \boldsymbol{l}_{s}' \\
    (\bar{\alpha}_t(s) + \alpha_{t-1}(s-2)) y_{\boldsymbol{l}_{s}'}^t & \text{otherwise} \\
\end{cases}
$$

where
$$
\bar{\alpha}_t(s) = \alpha_{t-1}(s) + \alpha_{t-1}(s-1)
$$

$$
\beta_t(s) =
\begin{cases}
\bar{\beta}_t(s)y_{\boldsymbol{l}_{s}'}^t & \boldsymbol{l}_{s}' = b \text{ or } \boldsymbol{l}_{s+2}' = \boldsymbol{l}_{s}' \\
(\bar{\beta}_t(s) + \beta_{t+1}(s+2)) y_{\boldsymbol{l}_{s}'}^t & \text{otherwise} \\
\end{cases}
$$

where
$$
\bar{\beta}_t(s) = \beta_{t+1}(s) + \beta_{t+1}(s+1)
$$

NOTE that the formula actually implies some rules of transitions in addition to the previous ones:

1. ...
2. ...
3. ...
4. TODO
5. TODO

As a result, the probability of $\boldsymbol{l}$, expressed using $\alpha$,
is the sum of the total probabilities
of $\boldsymbol{l}'$ with and without the final blank at time $T$:

$$
p(\boldsymbol{l}|\boldsymbol{x}) = \alpha_T(|\boldsymbol{l}'|) + \alpha_T(|\boldsymbol{l}'| - 1)
$$

Also, the probability of $\boldsymbol{l}$, expressed using $\beta$,
is the sum of the total probabilities
of $\boldsymbol{l}'$ with and without the first blank at time $1$:

$$
p(\boldsymbol{l}|\boldsymbol{x}) = \beta_T(1) + \beta_T(2)
$$

#### MLE


### Decoding
