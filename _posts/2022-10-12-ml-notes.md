---
title:  "Machine Learning Notes"
date:   2022-10-12 21:06:28 -0500
toc: true
permalink: /ml-notes
---

Machine Learning review notes

<!--more-->

# Fundamentals

## Logistic Regression

### Sigmoid

$$ y = \frac{1}{1 + e{-z}} $$

### Log Loss (Logistic Loss/Cross-Entropy Loss)

Log Loss is the negative average of the log of corrected predicted probabilities for each instance.

$$ L(y, p) = - \frac{1}{N} \sum_{i}^{N} (y_i\log(p_i) + (1 - y_i)\log(1 - p_i)) $$

True label $y \in \\{0, 1\\}$ and probability estimate $p$

**Can't use MSE loss** because when using Sigmoid+MSE the loss function w.r.t. weights $ L(w) $ become non-convex.

# Regularization

The intuitive difference between L1 and L2:
L1 tries to estimate the median of the data while L2 tries to estimate the mean of the data.

## L1 Regularization (Lasso Regression)

- Lasso = Least Absolute Shrinkage and Selection Operator
- L1 regularization leads to **sparsity** (some weight can be 0, so we can use it for **feature selection**)

$$ L = \sum_i^N(y_i - \boldsymbol{\omega}^T \boldsymbol{x}_i)^2 + \lambda ||\boldsymbol{\omega_i}||_1 $$

$$ ||\boldsymbol{\omega_i}||_1 = \sum_i^N |\boldsymbol{\omega}_i| $$

## L2 Regularization (Ridge Regression)

- L2 regularization deoes not have sparsity (some weight can be close to 0 but not 0)

$$ L = \sum_i^N(y_i - \boldsymbol{\omega}^T \boldsymbol{x}_i)^2 + \lambda ||\boldsymbol{\omega_i}||_2^2 $$

$$ ||\boldsymbol{\omega_i}||_2^2 = \sum_i^N |\boldsymbol{\omega}_i|^2 $$

## Priors of L1 and L2

Prior of the weight $ P(\omega) $

- follows a Laplace (Double Exponential) Distribution with mean 0 for Lasso Regression
- follows a Gaussian Distribution with mean 0 for Ridge Regression

## Useful Links

- https://ekamperi.github.io/machine%20learning/2019/10/19/norms-in-machine-learning.html
- https://ekamperi.github.io/mathematics/2020/08/02/bayesian-connection-to-lasso-and-ridge-regression.html

# Classic Machine Learning Models

## KNN

- Classification and regression
- Non-parametric
- Algorithm:
    1. Given a vector $v$, calculate the distance between it and every vector in the training data
    2. Sort the distances descendingly, keep the smallest $k$ samples
    3. For classification, the prediction of $v$ is the most common class labels in the $k$ neighbors. For regression,
       the prediction is the mean value of the neighbors.
- Applications: anomaly detection, search, recommender system

## k-means

- **k-means is a NP-hard problem, the k-means algorithm usually refers to Loyd's algorithm, a heuristic algorithm to
  solve this problem**
- k-means partitions observations into $k$ clusters in which each observation belongs to the nearest cluster
- These heuristic algorithms **don't guarantee to find the global optimum**. The result depend on the initial clusters
- Loyd's algorithm (*the* k-means algorithm)
    1. Determine $k$ and initialize $k$ clusters (determining the mean of each cluster $\mu_k$) in some way
    2. E-step: Compute the sum of the squared distances between each data point and all centroids, and assign each
       data point to the closest cluster (centroid)
    3. M-step: Compute the new centroid (mean) by taking the average of the all data points that was assigned to
       this cluster
    4. Keep iterating until the end condition is met (for example, max number of iterations finished)
- k-means++ (better initialization)
    1. Choose one data point as the initial center $c_1$ uniformly at random from the data samples
    2. For each data point $x$ not chosen, compute the distance $D(x)$ between it and the nearest center that has
       already been chosen
    3. Choose one new data point at random as a new center, the probability of a point $x'$ being chosen is
       $\frac{D(x)^2}{\sum_x D(x)^2}$
    4. Repeat until $k$ centers have been chosen
    5. Proceed to the standard k-means algorithm
- Applications: Vector quantization for signal processing (where k-means was originally developed), cluster analysis,
  feature learning, topic modeling

## Bagging

- Bagging = bootstrap aggregating
- Designed to improve the stability and accuracy of ML algorithms. It reduces variance and helps to avoid overfitting
- Sample with replacement to create different data subsets (bootstraps), and train a model on each of these bootstraps
    - Sampling with replacement ensures each bootstrap is independent of its peers
- The final prediction is the majority vote or average of all models' predictions
- Bagging generally improves unstable methods, such as neural networks, classification and regression trees, and subset
  selection in linear regression
- It can mildly degrade the performance of stable methods such as KNN
- Example: Random Forest

## Boosting

- Boosting is a family of iterative ensemble algorithms that convert weak learners to strong ones
- Start by training the first weak classifier on the original dataset
- Samples are re-weighted based on how well the first classifier classifies them: misclassified samples are given higher
  weight
- Train the second classifier on this re-weighted dataset. The ensemble now includes both classifiers
- Samples are re-weighted based on how well the ensemble classifies them.
- Repeat for as many iterations as needed. And the final strong classifier is created as a weighted combination of the
  existing classifiers (classifiers with smaller training errors have higher weights)
- Example: Gradient-booted Tree (GBT)
